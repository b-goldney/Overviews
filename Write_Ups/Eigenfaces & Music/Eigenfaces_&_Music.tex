\documentclass{cup-pan}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\title{Eigenfaces \& Music Genre Identification}

\author[1]{\normalsize Brandon Goldney}

\affil[1]{Department of Applied Mathematics, University of Washington, Email: \url{Goldney@uw.edu}}

\begin{document}
\maketitle

\begin{abstract}
	The purpose of this paper is twofold: i) to explore the Yale Faces Database and demonstrate the effect of eigendecomposition on those images and ii) explore the effects of singular value decomposition (SVD) on music.  Part 1 of this paper explores the strange but effective methods of facial recognition by converting images into greyscale and then applying PCA on the dataset.  Part 2 of the paper attempts to classify pieces of music based off spectrograms of sample music.  
\end{abstract}

\section{Section I: Introduction and Overview}
\vskip 0.5 cm
\noindent
\textbf{Part 1}
The accuracy of facial recognition algorithms has significantly increased in recent years.  Large companies such as Facebook and Google are leveraging the technology to better serve their customers and strealime certain processes.  For example, Facebook has functionality to automaically recognize friends in images, and Google allows users to search for an image (e.g. the Empire State Building) in hopes of correctly identifying it and providing relevant information.  

\vskip 0.5 cm
\noindent
\textbf{Part 2}
Advancements in machine learning have been making impacts in the music industry for several years.  For example, currently up for debate is if someone has the right to trademark a song or melody generated by machine learning.  Music classification is also important to companies such as Spotify, Apple, and Amazon as they rush to provide customized playlists and music reccommendations through their respective platforms.

\vskip 0.5 cm
\section{Section II: Theoretical Background}

\vskip 0.5 cm
\noindent
\textbf{Part 1}
\nonindent
"PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on." (source: \href{https://en.wikipedia.org/wiki/Principal\_component\_analysis}{Wikipedia}).  In order to maximize variance, the first weight vector $w_1$ satisfies:
$$
w_1 = arg max{\Sigma_i (t_1)^2_i} = arg max {\Sigma(x_i \dot w)^2}
$$

\noindent
The general process for applying PCA is the following:
\begin{enumerate}[noitemsep]
	\item After acquiring data, standardize it by subtracting the mean\\
	\item Calculate the covariance matrix\\
	\item Calcualte the eigenvectors and eigenvalues of the covariance matrix (SVD is the preferred method)\\
	\item Choose components and form a new feature vector\\
	\item Calculate the new data, where $Final\_Data = Row\_Feature\_Vector x Row\_Data\_Adjustment$ and $Row\_Feature\_Vector$ is the matrix with the eigenvectors in the columns transposed so the eigenvectors are in the rows, and $Row\_Data\_Adjustment$ is the mean adjusted data transposed.\\ 
\end{enumerate}
	
\vskip 0.5cm
\noindent
\textbf{Part 2}
\noindent
The G치bor Filter shows the frequency of a signal in the time domain.  This is an advancement from the Fourier Transform which shows a signal's amplitude in the frequency domain. In order to localize with respect to both time and frequency Gabor introduced a kernel:
$$g_{t, \omega}(\tau) = e^{i \omega \tau}g(\tau - t)$$

\vskip 0.1in
\noindent
Incorporating Gabor's kernel with the Fourier Transform results in the following:
$$G[f](t,\omega) = \tilde{f}_g(t,\omega) = \int_{-\infty}^{\infty} \space f(\tau) \bar{g}(\tau - t) \space e^{i \omega \tau} d \tau \space = \space (f, \bar g_{t,\omega})$$

\vskip 0.1in
\noindent
The bar denotes the complex conjugate.  While this formula appears intimidating at first, it can be broken down into its components to better understand it.  Notice, we're integrating over the term $\tau$, which effectively slides the window over the signal.  The new term, $\bar g (\tau - t)$ was conceptually explained earlier, it localizes the signal with respect to time and frequency.  The final term, $e^{-i \omega \tau}$, is the standard Fourier Transform.

\vskip 0.1in
\noindent
The G치bor transform is computed by discretizing the time and frequency domain.  Consider the following sample points:
$$ v = m \omega_0 \\
\tau = n t_0$$
where $m$ and $n$ are integers and $w_0$, $\tau_0$ > 0 are constants. In this case, the discrete version of G치bor's kernel becomes:
$$g_{m,n}(t) = e^{i 2 \pi m \omega_0 t} g(t-n t_0)$$
Accordingly, the G치bor Transform becomes:
$$\tilde{f}(m,n) = \int^{\infty}_{-\infty} f(t) \bar g_{m,n}(t) dt = (f,g_{m,n})$$
\vskip 1 cm

\vskip 0.05 cm
\noindent

\section{Section III:  Algorithm Implementation and Development}
\vskip 0.5 cm
\noindent
\textbf{Part 1}
\noindent
Due to the size of the dataset, approximately 2,414 images, we spend some time upfront ensuring the data is in a clean and easy to handle format.  There are 40 folders, each with 20+ images.  The first step is to utilize the \emph{glob} package in order to create a list of every file name.  Once the file names are loaded we can begin processing the data and preparing it for principal components analysis. 
\noindent
We start by converting each image into an n-dimensinal array, where each column represents the \emph{i'th} face.  Each face has $192 x 168$ pixels, translating into a $(32256,1)$ array after it is reshaped.  After we have the n-dimensional array setup we can normalize the data by subtracting the mean. The next step is to begin PCA on the covariance matrix.  Once we have the PCA components, we can then project new data (i.e. attempt to duplicate the faces in the dataset).  

\vskip 0.5 cm
\noindent
\textbf{Part 2}
\noindent
After loading the data, the first step is to define the length of the data (i.e usually 5 seconds in this case) and then to discretize it into $n$ data points. Next, we rescale the n data points by $(2* \pi)/L$, where $L$ equals the duration of the song. $L$ is calculated by dividing the number of data points by the sampling frequency. We know in later steps that we will be performing a Fourier Transform, which rearranges $X$ by shifting the zero-frequency component to the center of the array. With that in mind, we shift the $n$ data points prior to any further steps. Now that we have the initial variables declared and created the discretized points , we can begin working with the data from the sound file by setting up a for loop, where the window is created and the Fourier Transform is taken on the data.

Additionally steps were also take in an effort to leverage a spectrogram to classify songs from the Beatles, such as taking the SVD of the spectrogram of songs versus themselves.  After that approach was not successful, a Random Forest was applied to a series of spectrograms.  This created features which could then be used to categorize songs' spectrograms against.  This approach had mild success. 

\section{Section IV: Computational Results}

\vskip 0.5 cm
\noindent
\textbf{Part 1}
\vskip 0.5 cm
\noindent
Since we're familiar with PCA at this point, we will pick-up where things can become strange but effective.  For example, after subtracting the mean from each image, we can plot the "average" face (shown below). 

\begin{center}
\includegraphics[
  width=8cm,
  height=8cm,
  keepaspectratio,
]{Avg_Face.png}
\end{center}

\noindent
Additionally, we can use a scree plot to show us how many principal components are needed in order to explain a sufficient amount of variance in order to discern someone's face from a blur of color.  The scree plot is shown here:

\begin{center}
\includegraphics[
  width=8cm,
  height=8cm,
  keepaspectratio,
]{scree_plot.png}
\end{center}

\noindent
Also of interest and commonly referenced are the eigenfaces, these are the faces which explain the most variance.  Interestingly, it's almost possisble to see the flexibility of PCA.  We can see how the nose is mostly in place, but also slightly distorted so it can capture the variance for other people's noses.  Similar distortions can be seen in the eyes, mouth, forehead, and complexion. 

\begin{center}
\begin{figure}[h]
\begin{tabular}{ll}
\includegraphics[scale=0.4]{eigenface1.png}
&
\includegraphics[scale=0.4]{eigenface2.png}
\end{tabular}
\caption{Left: Eigenface 1 \; \; \; \; \; Right: Eigenface 2}
\label{Fig:Race}
\end{figure}
\end{center}

\vskip 0.5 cm
\noindent
\textbf{Part 2}
\noindent
To increase chances of success for this part, we leveraged Ben Frederickson's latent semantic analysis to compare bands.  Subsequently the last.fm dataset (more often called the Million Song Dataset) was used to pick the bands and songs. In this section, we'll use The Beatles, The Rolling Stones, and Bob Dylan as the "base bands" to make future comparisons. 

\vskip 0.5 cm
\noindent
The first approach to tackling this problem was to create a spectrogram, then search for certain patterns that are reflective of a specific band and/or genre.  However, this approach ran into several problems.  For example, the Beatles were notorious for being ahead of their time, thus easily conflating their music with music from other periods.  Additionally, music underwent a lot of change from the 60s, 70s, and 80s, increasing the complexity as well. A spectrogram of the Beatles' "Yellow Submarine" is shown below.

\begin{center}
\includegraphics[
  width=8cm,
  height=8cm,
  keepaspectratio,
]{song_spec.jpeg}
\end{center}
Despite reasonable efforts to categorize music by leveraging the spectrogram, no meanigful results were achieved.  However, some success was reached using a random forest. For this aspect, the Librosa library from Python was utilized.  
\begin{center}
\includegraphics[
  width=8cm,
  height=8cm,
  keepaspectratio,
]{song_categories.png}
\end{center}
\section{Section V: Summary and Conclusions}


\vskip 0.5 cm
\noindent
\textbf{Part 1}
\noindent
PCA has proven to be surprisingly effective at reconstructing faces and providing a reasonably degree of accuracy.  It is important to note, while the faces appear blurry to humans, they are less blurry to facial recognition algorithms.  The reasoning is because those algorithms often use metrics such as the distance between the nose and mouth, or distance between eyes.  As long the facial features are not distorted or spacing is perturbed, the coloring and smoothing can be materially insufficient before it affects an algorithims ability to recognize someone's characteristics.  

\vskip 0.5 cm
\noindent
\textbf{Part 2}
\noindent
Despite having mixed results for music classification, SVD could prove valueable for music with more consistent tune than the Beatles.  An area of further exploration is leveraging a Random Forest, which was adept at identifying features and bucketing songs accordingly. 

\section{Appendix A: Python functions used and brief implementation explanation}

\begin{itemize}
	\item np.reshape: Gives a new shape to an array without changing its data \\
	\item np.zeros: Return a new array of given shape and type, filled with zeros\\
	\item sklearn.pca: Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD
	\item pd.read\_table: Read general delimited file into DataFrame.
	\item sklearn.RandomForestClassifier: A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting
\end{itemize}
\noindent

\section{Appendix B: Python code}
\noindent
(see next page)
\end{document}
