{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The purpose of this notebook is to demonstrate the various matrix decomposition methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cell below is needed to enable equation numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "- The notes for this section, with the exception of section 2.1, are mostly comprised of a lecture series given by Steve Brunton [SVD: Mathematical Overview](https://www.youtube.com/watch?v=nbBvuuNVfco)\n",
    "- We'll first do a manual SVD calculation to refresh our memory from our introductory linear algebra class and then develop a more intuitive understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD for 2x2 matrix\n",
    "- [MIT: Computing the Singular Value Decomposition](https://www.youtube.com/watch?v=cOUTpqlX-Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the SVD for matrix $c$.\n",
    "$$\n",
    "\\begin{equation}\n",
    "    c = \\begin{bmatrix} \n",
    "            5 & 5 \\\\ \n",
    "            -1 & 7\n",
    "        \\end{bmatrix} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Recall, we are trying to set $c = U \\Sigma V^T$. $U$ and $V$ are going to be orthogonal matrices (i.e. their columns are orthonormal sets), and $\\Sigma$ is going to be a diagonal matrix. To begin solving this we're going to need two equations:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    C^TC = V \\Sigma^T \\Sigma V^T \\\\\n",
    "    CV = U \\Sigma\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "**Step 1: solve for $C^TC$ first:**\n",
    "$$\n",
    "\\begin{align}\n",
    "    c^Tc &= \\begin{bmatrix} \n",
    "            5 & -1 \\\\ \n",
    "            5 & 7\n",
    "        \\end{bmatrix} \n",
    "        \\begin{bmatrix} \n",
    "            5 & 5 \\\\ \n",
    "            -1 & 7\n",
    "        \\end{bmatrix} \\\\\n",
    "        &= \\begin{bmatrix}\n",
    "            26 & 18 \\\\\n",
    "            18 & 74 \n",
    "         \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " **Step 2: find the eigenvalue of $c^Tc$**\n",
    "$$\n",
    "\\begin{align}\n",
    "    det(c^Tc - \\lambda I) &= det \\left(\\begin{bmatrix}\n",
    "            26- \\lambda & 18\\\\\n",
    "            18 & 74 - \\lambda\n",
    "         \\end{bmatrix} \\right) \\\\\n",
    "         &= \\lambda^2 - 100\\lambda + 1600 \\\\\n",
    "         &= (\\lambda - 20) (\\lambda-80) \\\\\n",
    "         &\\therefore \\lambda_1 = 20, \\, \\lambda_2 = 80\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now we need to find the eigenvectors for $\\lambda = 80$ and $\\lambda = 20$. Remember, by convention, we always do the most significant lambda value first - that way it appears as the first column in the matrix $v$.\n",
    "\n",
    "**Step 3: find the eigenvectors corresponding to the eigenvalue of $\\lambda = 80$**  \n",
    "Subtract $\\lambda = 80$ from the diagonal entries.\n",
    "$$\n",
    "\\begin{align}\n",
    "    c^Tc - 80\\lambda I &= \\left(\\begin{bmatrix}\n",
    "            -54 & 18 \\\\\n",
    "            18 & -6 \n",
    "         \\end{bmatrix} \\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Let's reduce this matrix to find $v_1$.  If we do the following transformations: $R_1 \\rightarrow R_1 / -54$ and then $R_2 \\rightarrow R_2 - 18R_1$ we'll end up with the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "        1 & -0.33 \\\\\n",
    "        0 & 0\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "Multiplying that matrix by the vector $\\left[ x_1, x_2 \\right]^T$ we'll end up with $x_1 - 0.33 x_2 = 0$ which implies $x_1 = 0.33x_2$. Therefore, by letting $x_2 = 1$, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "    v = \\begin{bmatrix}\n",
    "            0.33 \\\\\n",
    "            1\n",
    "        \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 4: find the eigenvectors corresponding to the eigenvalue of $\\lambda = 20$**  \n",
    "Subtract $\\lambda = 80$ from the diagonal entries.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "c^Tc - 20I = \\begin{bmatrix}\n",
    "            6 & 18 \\\\\n",
    "            18 & 54\n",
    "         \\end{bmatrix} \n",
    "\\end{align}\n",
    "$$\n",
    "Let's reduce this matrix to find $v_2$.  If we do the following transformations: $R_1 \\rightarrow R_1 / 18$ and then $R_2 \\rightarrow R_2 - 6R_1$ we'll end up with the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "c^Tc - 20I = \\begin{bmatrix}\n",
    "            1 & 3 \\\\\n",
    "            0 & 0\n",
    "         \\end{bmatrix} \n",
    "\\end{align}\n",
    "$$\n",
    "Multiplying that matrix by the vector $\\left[ x_1, x_2 \\right]^T$ we'll end up with $x_1+ 3x_2 = 0$ which implies $x_1 = -3x_2$. Therefore, by letting $x_2 = 1$, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "    v = \\begin{bmatrix}\n",
    "            -3 \\\\\n",
    "            1\n",
    "        \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 5: Normalize $v_1$ and $v_2$**  \n",
    "For $v_1$ we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "    Length \\; L = \\sqrt{0.33^2 + 1^2} = 1.05 \\\\\n",
    "    v_1 = \\left(\\frac{0.33}{1.05}, \\frac{1}{1.05}\\right) = (0.32, 0.95)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And for $v_2$ we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "    Length \\; L = \\sqrt{-3^2 + 1^2} = 3.16\\\\\n",
    "    v_2 = \\left(\\frac{-3}{3.16}, \\frac{1}{3.16}\\right) = (-0.95, 0.32)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 6: We have matrices $\\Sigma$ and $U$**  \n",
    "$$\n",
    "\\begin{align}\n",
    "    \\Sigma = \\begin{bmatrix}\n",
    "            \\sqrt{80} & 0 \\\\\n",
    "            0 & \\sqrt{20}\n",
    "        \\end{bmatrix} = \n",
    "        \\begin{bmatrix}\n",
    "            8.94 & 0 \\\\\n",
    "            0 & 4.47\n",
    "        \\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "    v = [v_1, v_2] =  \\begin{bmatrix}\n",
    "                                0.32 & -0.795 \\\\\n",
    "                                0.95 & 0.32\n",
    "                             \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 7: Find $U$ via the following formula: $\\dfrac{1}{\\sigma_i}A \\,v_i$**\n",
    "$$\n",
    "\\begin{align}\n",
    "U = \\dfrac{1}{\\sigma_i}A \\,v_i &= \\dfrac{1}{80} \\, \n",
    "                                    \\begin{bmatrix} \n",
    "                                        5 & 5 \\\\ \n",
    "                                        -1 & 7\n",
    "                                    \\end{bmatrix} \n",
    "                                     \\begin{bmatrix}\n",
    "                                        0.32 & -0.95 \\\\\n",
    "                                        0.95 & 0.32\n",
    "                                     \\end{bmatrix} \\\\\n",
    "                           &= \\begin{bmatrix}\n",
    "                                   0.71 & -0.71 \\\\\n",
    "                                   0.71 & 0.71\n",
    "                              \\end{bmatrix}                              \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 8: Confirm that $U = (U*\\Sigma) * V^T$**\n",
    "- We'll confirm this with numpy in the step below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the solution above\n",
    "u = [[1/np.sqrt(2), -1/np.sqrt(2)], [1/np.sqrt(2), 1/ np.sqrt(2)]]\n",
    "sigma = [[np.sqrt(80),0],[0, np.sqrt(20)]]\n",
    "v = [[0.32, 0.95], [-0.95, 0.32]] # note it's V^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.02802148,  4.9963987 ],\n",
       "       [-0.98030607,  7.02025641]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.matmul(u,sigma)\n",
    "np.matmul(temp, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving SVD with numpy\n",
    "- Notice, the results match what we calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00, 5.00],\n",
       "       [-1.00, 7.00]])"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [[5,5],[-1,7]]\n",
    "u, s, vh = np.linalg.svd(c,full_matrices=True)\n",
    "s = [[8.94,0],[0,4.47]] # Need to redefine s because np.linalg only provides the eigenvalues (not the full matrix)\n",
    "\n",
    "temp = np.matmul(u,s) # create temp matrix\n",
    "np.matmul(temp,vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71 0.71]\n",
      " [0.71 -0.71]] \n",
      " \n",
      " [[8.94, 0], [0, 4.47]] \n",
      " \n",
      " [[0.32 0.95]\n",
      " [0.95 -0.32]]\n"
     ]
    }
   ],
   "source": [
    "print(u, \"\\n \\n\", s, \"\\n \\n\", vh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $SVD$ for says that for any matrix $A$,\n",
    "$$\\begin{equation}\n",
    "A = U \\Sigma V^T\n",
    "\\end{equation}$$\n",
    "where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a diagonal matrix. \n",
    "If $A$ is an invertible matrix then we can use its $SVD$ to find $A^{-1}$:\n",
    "$$\\begin{equation}\n",
    "A^{-1} = V\\Sigma^{-1}U^*\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly review some reasons why we care about $SVD$:  \n",
    "- SVD works on non-square matrices\n",
    "- SVD is used in principal components analysis (PCA)\n",
    "- Where is it used? Google page rank algorithm, facial recognition technology, recommender systems (e.g. Netflix, Amazon)\n",
    "- Why is it so popular? The SVD is based on simple, interpretable linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's review some properties and features of the $SVD$:\n",
    "- SVD is guaranteed to exist and it is unique\n",
    "- The $U$ and $V$ matrix are orthogonal (i.e. unitary matrices), and the $\\Sigma$ matrix is diagonal\n",
    "- \"The columns of U are the left singular vectors; S (the same dimensions as A) has singular values and is diagonal (mode amplitudes); and VT has rows that are the right singular vectors. The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal.\" - [MIT: SVD tutorial](https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review some key features about each individual matrix. Recall, the advantage of $SVD$ is that it is has intuitive and physical interpretations.\n",
    "- $U$ Matrix  \n",
    "    - Unitary matrix\n",
    "    - $UU^T = U^TU = I_{n x n}$\n",
    "    - This is called the left-singular vector  \n",
    "    - **Interpretation:**\n",
    "    \n",
    "\n",
    "- $\\Sigma$ Matrix  \n",
    "    - Diagonal matrix with the same shape as $X$ matrix\n",
    "    - $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge ... \\ge \\sigma_m \\ge 0$\n",
    "        - This means each column of eigenvectors is \"more important\" than the column to the right. In other words, each column of $\\Sigma$ provides a basis in which we can represent $X$\n",
    "    - This is called a matrix of singular values  \n",
    "    - **Interpretation:** the columns are hierarchically organized in order of importance \n",
    "\n",
    "\n",
    "- $V$ Matrix  \n",
    "    - Unitary matrix\n",
    "    - $VV^T = V^TV = I_{m x m}$\n",
    "    - This is called the right-singular vector  \n",
    "    - **Interpretation:** This is best explained via example.  In the case of facial recognition (e.g. EigenFaces), the first column of $V^T$ is the mixture of all the faces from matrix $U$ that is needed to equal to equal the first column of $X$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes from [Singular Value Decomposition (SVD): Dominant Correlations](https://www.youtube.com/watch?v=WmDnaoY2Ivs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A key interpretation is the following: We can think of $U$ and $V^T$ as eigenvectors of a correlation matrix given by $XX^T$ or $X^TX$\n",
    "- The methodology outlined below is not a very efficient or accurate way to calculate the SVD; however, it provides an intuitive understanding of the SVD matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Matrix**  \n",
    "Let's assume that matrix $X$ is a tall skinny matrix. In the eigenfaces example, each column would represent a face.  Now, let's look at the underlying math:\n",
    "$$\n",
    "\\begin{align*}\n",
    "X^TX &=  \\begin{bmatrix} \n",
    "            \\ldots & x_1^T & \\ldots \\\\ \n",
    "            \\ldots & x_2^T & \\ldots \\\\ \n",
    "        \\end{bmatrix} \n",
    "        \\begin{bmatrix} \n",
    "            \\vdots & \\vdots  & \\vdots \\\\ \n",
    "             x_1 & x_2 & x_3 \\\\ \n",
    "            \\vdots & \\vdots  & \\vdots \\\\ \n",
    "            \\vdots & \\vdots  & \\vdots \\\\ \n",
    "        \\end{bmatrix} \\\\\n",
    "         &= \\begin{bmatrix} \n",
    "            x_1^Tx_2 & x_1^Tx_2 & \\ldots & x_1^Tx_m \\\\\n",
    "            x_2^Tx_2 & x_2^Tx_2 & \\ldots & x_2^Tx_m \\\\\n",
    "            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_m^Tx_2 & x_m^Tx_2 & \\ldots & x_m^Tx_m \\\\\n",
    "        \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This new matrix is the correlation matrix. \n",
    "\n",
    "Let's examine what each entry of this new matrix, $X^TX$, represents. Each entry in the new matrix is an inner product between the two matrices.  More specifically, we can write: $x_i^Tx_j = <x_i, x_j>$. Applying basic properties of inner products, we know that if the inner product between two faces is close to zero then the two faces are dissimilar, and if two faces have a large value then the two faces share a lot of features. \n",
    "\n",
    "Let's look further into what this new matrix represents. Recall,\n",
    "$$\n",
    "\\begin{equation}\n",
    "    X=U \\Sigma V^T \\\\\n",
    "    X^T = V \\Sigma U^T\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Multiplying $X^TX$ we receive:\n",
    "$$ \n",
    "\\begin{align}\n",
    "    X^TX &= V \\Sigma U^T U \\Sigma V^T \\\\\n",
    "    &= V \\Sigma^2 V^T\n",
    "\\end{align}\n",
    "$$ where we applied the following fact $U^T U = I $.\n",
    "\n",
    "Note, equation 3 is eigenvalue decomposition for the matrix $X^TX$.  Let's multiply the LHS of equation 2 and equation 3 by $V$ to cancel out the $V^T$.\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^TXV = V \\Sigma^2 \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now, the $\\Sigma^2$ values are the eigenvalues and the $V$ term is the eigenvectors of the correlation matrix (i.e. $X^TX$).\n",
    "Next steps: We're going to compute $U, \\Sigma, V$ as the eigenvalues and eigenvectors of the correlation matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LU Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cholesky decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "377.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495.646px",
    "left": "1548.33px",
    "right": "20px",
    "top": "125px",
    "width": "367.312px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
