\documentclass{cup-pan}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{cite}

\title{Principal Components Analysis (PCA)}

\author[1]{\normalsize Brandon Goldney}

\affil[1]{Department of Applied Mathematics, University of Washington, Email: \url{Goldney@uw.edu}}

\begin{document}
\maketitle

\begin{abstract}
	The purpose of this paper is to demonstrate the effectiveness of principal components analysis.  PCA is applied on a series of four  different experiments, each experiment being comprised of three videos. In all four experiments a paint can is recorded as it is displaced and  simultaneously recorded by three different cameras. This is analogous to the classic mass-spring system in physics.  The objective is to track the paint can and apply PCA to reduce noise which is introduced throughout the videos.  
\end{abstract}

\section{Section I: Introduction and Overview}
Each of the four experiments in this paper pose a unique challenge. A brief overview of each experiment is provided here:  
\begin{enumerate}
	\item \textbf{ Ideal Case} - A small displacement of the mass in the $z$ direction and the ensuing oscillations. In this case, the entire motion is in the $z$ directions with simple harmonic motion being observed.
	\item \textbf{Noisy Case} -  Repeat the ideal case experiment, but this time, introduce camera shake into the video recording. This should make it more difficult to extract the simple harmonic motion. But if the shake isn’t too bad, the dynamics will still be extracted with the PCA algorithms
	\item \textbf{Horizontal Displacement} - In this case, the mass is released off-center so as to produce motion in the $x−y$ plane as well as the $z$ direction. Thus there is both a pendulum motion and a simple harmonic oscillations.
	\item \textbf{Horizontal Displacement and Rotation} - In this case, the mass is released off-center and rotates so as to produce motion in the $x−y$ plane, rotation as well as the $z$ direction. Thus there is both a pendulum motion and a simple harmonic oscillations.
\end{enumerate}

\vskip 0.1in
\noindent
In each case, there are three cameras in different locations recording the movement of the paint can. Each camera provides a standard colorized image.  We'll transform the RBG pixels of each frame to grey scale and utilize pixel intensity to track the coordinates of the paint can.  Leveraging that information, we will reduce the dimensionality of the problem and project the coordinates of the paint can onto the new orthonormal bases.

\vskip 0.1in
\noindent

\section{Section II: Theoretical Background}
PCA can be applied using Singular Value Decomposition (SVD) or eigendecomposition. The eigendecomposition method will be briefly explained as it provides a clear and intuitive approach to solving PCA. 

\vskip 0.05in
\noindent
\textbf{Eigendecomposition}
The principle components can be thought of in terms of the covariance matrix.  The diagonal entries of the covariance matrix are the variances for the $i^{th}$ feature; therefore, terms with a largest magnitude explained the most variance. Similarly, terms on the diagonal with smaller magnitude explain less variance. Extending that understanding to terms in the off-diagonal, terms with a large magnitude reflect two features which have a high degree of redundancy.  It logically follows that the dimensionality of the system can be reduced to those features which can explain the most variance (i.e. largest terms on the diagonal). 

\vskip 0.05in
\noindent
The covariance matrix is
\begin{equation}
\label{eq:1}
	C_x = \dfrac{1}{n-1}XX^T
\end{equation}
"where the matrix $X$ contains the data from the experiments, and $X \in C^{mxn}$ where m is the number of the probes or measuring positions, and n is the number of experimental data points."

\vskip 0.05in
\noindent
Subsequently, the covariance matrix can be diagonalized by acknowledging the fact that $XX^T$ is a square, symmetric \emph{m} x \emph{m} matrix.  
\begin{equation}
\label{eq:2}
	XX^T = S \Lambda S^{-1}    
\end{equation}

\noindent
Additionally, since it is a symmetric matrix the $S$ can be written as a unitary matrix with $S^{-1} = S^T$, and $\Lambda$ is a diagonal matrix whose entries correspond to the $m$ distinct eigenvalues of $XX^T$.   Therefore, we can write the following:
\begin{equation}
\label{eq:3}
	Y=S^TX
\end{equation}

\noindent
Now that we're in a new basis, we can calculate the covariance.
\begin{flalign*}
	C_Y &= \dfrac{1}{n-1}YY^T \\
	&= \dfrac{1}{n-1}\\
	&= \dfrac{1}{n-1}S^T(XX^T)S\\
	&= \dfrac{1}{n-1} \Lambda
\end{flalign*}

\vskip 0.05in
\noindent
In the new form, the principal components are the eigenvectors of $XX^T$.  

\vskip 0.05in
\noindent
\textbf{SVD}

\vskip 0.05in
\noindent
Based off that (hopefully) intuitive understanding of principal components, we'll look into SVD in greater detail.  

\noindent
By leveraging the appropriate pair of bases $U$ and $V$, SVD can diagonalize any matrix.  
\begin{equation}
	\label{eq:4}
	Y=U*X
\end{equation}

\noindent
where $U$ is the unitary transformation associated with the SVD: $X=U \Sigma V*$.  In this new form, we can calculate the variance:

\begin{flalign*}
	\label{eq:5}
	C_Y &= \dfrac{1}{n-1} YY^T \\
	&= \dfrac{1}{n-1}(U*X)(U*X)^T \\
	&= \dfrac{1}{n-1}U^*(XX^T)U \\
	&= \dfrac{1}{n-1} U*U \Sigma ^2 UU^*\\
	&= \dfrac{1}{n-1} \Sigma^2
\end{flalign*}

\noindent
We can see that $\Sigma^2 = \Lambda$ from the eigendecomposition method.  
	
\vskip 0.1in
\noindent
\section{Section III:  Algorithm Implementation and Development}

\vskip 0.1in
\noindent
The PCA implementation in these experiments can be viewed as having two steps.  The first step is to process the data into a digestable format so the coordinates of the paint can are able to be easily tracked.  The second step is the actual PCA component, reducing the dimensionality of the 3 videos for each experiment.  

\noindent
\textbf{Step 1: Processing data}\\
\noindent
We'll mainly use the first test case as the example because the methodology is nearly identical in the other cases.  First, we need to determine the coordinates of the paint can in the first picture.  We do this in Matlab, leveraging a built-in GUI allowing the user to click on an area and the GUI returns the $x$ and $y$ coordinates. Subsequently, we convert each image from RGB values to greyscale.  The advantage of this is that it allows us to focus on one number rather than three (i.e. RGB values).  Also, the paint can has a flashlight fixed to the top of it; therefore, we're able to simply extract the maximum value from the area around the paint can in order to ascertain the coordinates.  

\vskip 0.05in
\noindent
This raises a new issue: the flashlight is not always the brightest pixel in the image, bringing us back to the first step where we extracted the coordinates of the flashlight.  The advantage of manually extracting the first image is that we're able to set a window around the flashlight and set all other values to zero (i.e. black).  In other words, we determine the coordinates of the flashlight in the first image, set a window of 20 pixels around the coordinates, then set all other pixels to zero.  We then search the next image within that window, and then update the coordinates of the window, and repeat the process. This step is important to reduce computing time, as well as accurately tracking the paint can. An example of this step is shown below.

\begin{figure}[h]
\begin{tabular}{ll}
\includegraphics[scale=0.4]{Paint_Can.png}
&
\includegraphics[scale=0.4]{Paint_Can_window.png}
\end{tabular}
\caption{Left: Normal Image \; \; \; \; \; Right: Image with window applied}
\label{Fig:Race}
\end{figure}

\vskip 0.1in
\noindent
\textbf{Step 2: PCA}\\
The process for calculating the principle components is fairly straightforward.  We'll walk through the implementation of the algorithm, but defer topics pertaining to the "why" to Section 1. 
\noindent
After normalizing the data, the covariance matrix is calculated.  Subsequently, the SVD of the covariance matrix is calculated.  At this point, we have all the information we need.  The final step is to manipulate the data so it's in a digestible format.  To better understand PCA, we look at the explained variance, which is similar to an $R^2$ score in linear regression.  Explained variance shows the amount of variance explained by each component compared to the total variance.   

\section{Section IV: Computational Results}
The computational results are most easily viewed through the lens of explained variance.  However, prior to viewing the output, it's helpful to view the movement of the paint can along the $x$ and $y$ axis.  

\noindent
The Ideal Case and Noisy Case (i.e. tests 1 and 2) are shown first then the Horizontal Displacement and Horizontal Displacement with Rotation (i.e. test 3 and 4).  

\vskip 0.05in
\noindent
\textbf{Ideal Case and Noisy Case: x-axis}
\begin{center}
\includegraphics[
  width=13cm,
  height=8cm,
  keepaspectratio,
]{test1_x.png}
\end{center}
\begin{center}
\includegraphics[
  width=13cm,
  height=8cm,
  keepaspectratio,
  ]{test2_x.png}
\end{center}

\noindent
As expected in the Ideal Case, the x-axis has minimal volatility as the paint can was only moved in the vertical direction.  However, notice the third camera demonstrates harmonic motion.  This is because the camera was turned 90 degrees - this is evidenced by the relatively flat line for the y-axis (shown below). Additionally, we can see in the Noisy Case that the x-axis demonstrates some more volatility, due to noise being introduced.  

\vskip 0.05in
\noindent
\textbf{Ideal Case and Noisy Case: y-axis}
\begin{center}
\includegraphics[
  width=13cm,
  height=8cm,
  keepaspectratio,
]{test1_y.png}
\end{center}
\begin{center}
\includegraphics[
  width=13cm,
  height=8cm,
  keepaspectratio,
  ]{test2_y.png}
\end{center}

\noindent
Again, we see the y-axis for the Noisy Case shows more volatility. 

\vskip 0.05in
\noindent
\textbf{Horizontal Displacement and Horizontal Displacement with Rotation: x-axis}

\begin{center}
\includegraphics[
  width=13cm,
  height=8cm,
  keepaspectratio,
]{test3_x.png}
\end{center}
\begin{center}
\includegraphics[
  width=13cm,
  height=8cm,
  keepaspectratio,
  ]{test4_x.png}
\end{center}

\noindent
As the name suggests, in the Horizontal Displacement case  we can see simple harmonic motion in the x-axis. In the case for Horizontal Displacement with Rotation, we can see much more noise, as the camera is not always readily observable.  

\vskip 0.05in
\noindent
\textbf{Horizontal Displacement and Horizontal Displacement with Rotation: y-axis}

\begin{center}
\includegraphics[
  width=13cm,
  height=8cm,
  keepaspectratio,
]{test3_y.png}
\end{center}
\begin{center}
\includegraphics[
  width=13cm,
  height=8cm,
  keepaspectratio,
  ]{test4_y.png}
\end{center}

\vskip 0.05in
\textbf{Explained Variance}
\begin{center}
\includegraphics[width=.4\textwidth]{test1_exp_var.png}
\includegraphics[width=.4\textwidth]{test2_exp_var.png}
\includegraphics[width=.4\textwidth]{test3_exp_var.png}
\includegraphics[width=.4\textwidth]{test4_exp_var.png}
\end{center}
\caption{Explained Variance for each case}

\noindent
We can clearly see, as noise is introduced into the system, it takes more principal components to explain 95\% of the variation (95\% was arbritrarily chose as the threshold for success). 

\noindent
Additionally, we can project the data onto each of the principle components.  This reflects how the paint can traverses across each new basis. We can see as more noise is introduced, the variation of the third principle component becomes greater, as it explains more of the variance.  Only three principle components are shown to increase clarity of the graphs; however, more components can be added. 

\begin{center}
\includegraphics[width=.4\textwidth]{test1_proj.png}
\includegraphics[width=.4\textwidth]{test2_proj.png}
\includegraphics[width=.4\textwidth]{test3_proj.png}
\includegraphics[width=.4\textwidth]{test4_proj.png}
\end{center}
\caption{Explained Variance for each case}


\section{Section V: Summary and Conclusions}
This paper demonstrates the ability of PCA to successfully capture the dynamics of a system, despite moderate amounts of noise being introduced. In a variety of scenarios we were able to track a paint can that is moving semi-randomly, and apply PCA via singular value decomposition to reduce the dimensionality of the system.  The output of the results, was aligned with our expectations and consistent with expectations from a mathematical perspective.    

\vskip 0.1in
\noindent


\vskip 0.1in
\noindent

\section{Appendix A: Python and Matlab functions used and brief implementation explanation}
\textbf{Matlab}
This was the code used to click on the flashlight and retrieve the x and y axis.
\begin{itemize}
	\item figure(); imshow(vidFrames1(:,:,:,1); [x1, y1] = ginput(1)
\end{itemize}
\noindent
\textbf{Python}
\begin{itemize}
	\item rgb2gray: This example converts an image with RGB channels into an image with a single grayscale channel. \\
	\item np.unravel\_index: Converts a flat index or array of flat indices into a tuple of coordinate arrays. \\
	\item np.cov: Estimate a covariance matrix, given data and weights. \\
	\item np.linalg.svd: Singular Value Decomposition.\\ 
	\item np.matmul: Matrix product of two arrays.\\
\end{itemize}
\section{Appendix B: Python code}
\noindent
(see next page)
\end{document}
